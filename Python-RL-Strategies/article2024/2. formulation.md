
# 2. Formulação

Em sua formulação canônica, o problema da patrulha multi-agente foi definido como um problema de otimização combinatória.
A definição aqui tratada está intimamente ligada às definições de (Machado, 2003, Sampaio, 2010, etc). Porém, neste trabalho
ela precisa ser reformulada em algum dois modelos teóricos de base para a Aprendizagem por Reforço. Vamos iniciar
discutindo os modelos teóricos para problemas de um agente e, depois, passaremos para modelos adequados a múltiplos agentes.

Um problema de Aprendizagem por Reforço tradicional, mono-agente, costuma ser tratado como um Markov Decision Process (MDP)
ou um Partially-Observable Markov Decision Process. Estes modelos servem para representar tomadas de decisões sequenciais,
e são a base teórica para propor e estudar os algoritmos formalmente. Existem variações na definição (Stefano, ) mas
vamos definir um MDP finito como uma 5-tupla $(S, A, p, \mu_0, S_T)$ onde $S$ representa um conjunto de possíveis estados do
problema; $S_T$ representa estados terminais, ou seja, estados onde a isntância (chamada de *episódio*) do problema se encerra; $A$ representa o conjunto de ações que podem ser escolhidas pelo agente. O $p$ representa a dinâmica do problema
e descreve a probabilidade se se chegar a um estado $s'$ e receber uma recompensa $r$ como resultados de o agente realizar
a ação $a$ em um estado $s$. Usamos a notação $p(s',r|s,a)$ para representar essa probabilidade. Note que é uma probabilidade
conjunta para o par $(s',r)$ em função do par $(s,a)$. 

Assim, o agente interage com este problema realizando uma ação por vez, provocando uma mudança no estado do problema e gera alguma recompensa como resultado (conforme as probabilidades definidas por $p$). Caso o agente alcance um estado terminal
(ou seja, um estado de $S_T$) o *episódio* se encerra. Algumas tarefas são *non-ending* pois não têm nenhum estado terminal. Alternativamente, podemos considerar que, os estados terminais são estados em que toda ação leva a ele mesmo, com recompensa 0
e, dessa forma, tarefas episódicas podem ser vistas como casos especiais das tarefas *non-ending*. 

A forma do agente escolher a ação é formalmente definida por meio do conceito de *política*. Uma política $\pi$ representa as probabilildades de escolha (por parte do agente) de cada uma das possíveis ações em função do estado atual. A probabilidade de escolher uma a ação específica $a$ em um dado estado $s$ é representada como $\pi(a|s)$. Fazer um agente aprender esta 
política é o problema central tratado pelos algoritmos de controle da Aprendizagem por Reforço. 
**Mas, ainda precisamos definir como avaliar a qualidade de uma política.**
ou

**Vamos adiar a discussão sobre como avaliar a qualidade de uma política para a subseção a seguir**

Existem algumas medidas para avaliar um política em função das recompensas recebidas pelo agente a cada passo
de interação com o problema, representadas por $R_0$, $R_1$, $R_2$, etc.
A principal medida, usada na vasta maioria dos trabalhos recentes de aprendizagem por reforço, é chamada de *retorno descontado*
e é calculada pela fórmula abaixo, para um dado parâmetro do problema $\gamma$. A esperança é calculada tendo em vista
as probabilidades de $p$, $\pi$.

$$
\mathbb{E}\left[\sum_{t=1}^{\infty}{\gamma^{(t-1)} \times R_i}\right]
$$

O $gamma$ é um valor do intervalo $(0;1]$, que serve para dar maior peso às recompensas imediatas e diminuir progressivamente
os pesos das recompensas posteriores. 

A política ótima, na prática da área, costuma ser entendida como aquela que garante o *retorno descontado* máximo, a partir 
da distribuição de estados iniciais $\mu_0$.

Uma importante propriedade que um MDP deve obedecer é que  o agente tem acesso ao estado $s$ e que as probabilidades para
cada possível par (s',r) deve depender apenas do estado atual s e da ação a realizada, 
i.e. não depende do histórico anterior de estados e ações.
Uma consequência dessa propriedade é que a política ótima pode ser definida em função apenas do estado atual.

A seguir, vamos definir alternativas ou variantes do MDP que são mais adequadas para modelar o problema da patrulha multiagente
que queremos tratar aqui.

## POMDP

Em um Partially Observable MDP (POMDP), o agente tem acesso apenas a parte das informações do estado. Vamos
usar $o$ para representar a essa *observação* parcial a que ele tem acesso a cada passo e seja $h_t = o_0, o_1, \cdots, o_t$ o histórico de estados anteriores ao passo $t$. Em um POMDP, definimos $p$ e $\pi$ em função desse histórico de observações. 
Assim a probabilidade de atingir um estado $s_{t+1}$ com recompensa $r_{t+1}$ é definida por uma função 
$p(s_{t+1}, r_{t+1} | h_t, a_t)$.
A política do agente define a probabilidade  $\pi(a|h_t)$ de escolher uma ação $a$ qualquer em função do histórico $h_t$. 
No restante, o POMDP segue a mesma definição do MDP, inclusive quanto à medida a ser otimizada.


## Average-Reward Formulation

Uma formulação alternativa focada especificamente nas tarefas non-ending propõe uma função-objetivo alternativa sem descontos 
para avaliar uma política nos MDPs ou POMDPs. Ela define a medida abaixo:

$$
\lim_{{h \to \infty}} \left(\frac{1}{h} \sum_{t=1}^{h}(R_i - \overline{R})\right)
$$

Nos argumentamos que esta medida é mais adequada para o problema da patrulha multiagente, que é inerentemente um problema 
non-ending. Uma série de justificativas para preferir esta formulação no lugar da recompensa descontada pode ser derivada de,
em especial quando a solução envolve representar a política por meio de redes neurais.
- A patrulha multiagente,a formulação canônica de referência, é inerentemente non-ending, pois ela não tem um estado terminal
específico. O uso de recompensas descontadas traz dificuldades nestes caso, pois com $\gamma=1$, temos uma soma divergente;
e com $\gamma<1$, as recompensas posteriores têm menor peso, o que é obviamente inadequado para uma non-ending task.
- Com o uso de aproximação de função para representar a política, o problema fica ill-defined. Mais detalhes em (Suton,??).

## Lidando com Múltiplos Agentes



---

Estou pensando em chamar de formulação canônica à formulação como mero problema de otimização combinatória.
Quanto à formulação adequada para LR: 
Situações multiagentes para aplicar com LR (com recompensas que podem ser diferentes), em geral, devem ser tratados como POSG.
objetivo padrão de cada agente: retorno descontado
Se as recompensas forem comuns (a cada passo): Dec-MDP com average reward. Ver o livro de Stefano (2024), pag. 51. E ver livro de Oliehoek (2016), seção 6.1.2, pag. 68.
Saha (2014) é um artigo que usa POSG com objetivo dado como recompensa média, mas ele é bem complicado.
Vamos aproximar o problema como uma série de POMDPs de recompensa média. E vamos aplicar algoritmos adaptados para esse caso usando CTDE (centralized training, decentralized execution).
Argumentar que o uso é adequado para uma tarefa infinita como a patrulha.
